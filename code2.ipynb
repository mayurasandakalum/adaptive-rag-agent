{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown, clear_output\n",
    "\n",
    "def display_md(text):\n",
    "    clear_output()\n",
    "    display(Markdown(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def display_json(json_data):\n",
    "    # clear_output()\n",
    "    display(Markdown(f\"```json\\n{json.dumps(json_data, indent=2)}\\n```\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if \"GROQ_API_KEY\" not in os.environ:\n",
    "    # os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter your Groq API key: \")\n",
    "    os.environ[\"GROQ_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Makara\\.conda\\envs\\ai-agents-env\\Lib\\site-packages\\langchain_groq\\chat_models.py:362: UserWarning: WARNING! response_format is not default parameter.\n",
      "                    response_format was transferred to model_kwargs.\n",
      "                    Please confirm that response_format is what you intended.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "llm_json_mode = ChatGroq(\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    response_format={\"type\": \"json_object\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a special kind of agent that when i give you an question, you will get answer for that question by solve that in step-by-step.\",\n",
    "    ),\n",
    "    (\"human\", \"How many `r` letters in the word `strawberry`\"),\n",
    "]\n",
    "ai_msg = llm.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"To find the number of `r` letters in the word `strawberry`, let's break it down step by step:\\n\\n1. **Write down the word**: The word is `strawberry`.\\n2. **Separate the letters**: Separate each letter in the word to count them individually: `s-t-r-a-w-b-e-r-r-y`.\\n3. **Count the `r` letters**: Now, count how many times the letter `r` appears in the separated letters:\\n   - The first `r` is after `t`.\\n   - The second `r` is after `e`.\\n   - There is another `r` right after the second `r`.\\n4. **Conclusion**: There are **3** `r` letters in the word `strawberry`.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 162, 'prompt_tokens': 79, 'total_tokens': 241, 'completion_time': 0.827973507, 'prompt_time': 0.018752949, 'queue_time': 0.806259212, 'total_time': 0.846726456}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_c0cfa69934', 'finish_reason': 'stop', 'logprobs': None}, id='run-ca912c48-a142-4367-a74c-4efb2ee45296-0', usage_metadata={'input_tokens': 79, 'output_tokens': 162, 'total_tokens': 241})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To find the number of `r` letters in the word `strawberry`, let's break it down step by step:\n",
       "\n",
       "1. **Write down the word**: The word is `strawberry`.\n",
       "2. **Separate the letters**: Separate each letter in the word to count them individually: `s-t-r-a-w-b-e-r-r-y`.\n",
       "3. **Count the `r` letters**: Now, count how many times the letter `r` appears in the separated letters:\n",
       "   - The first `r` is after `t`.\n",
       "   - The second `r` is after `e`.\n",
       "   - There is another `r` right after the second `r`.\n",
       "4. **Conclusion**: There are **3** `r` letters in the word `strawberry`."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Display the response\n",
    "display(Markdown(ai_msg.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}\")\n",
    "        \n",
    "_set_env(\"TAVILY_API_KEY\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "_set_env(\"LANGSMITH_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"self-correcting-rag-system\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorestore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import SKLearnVectorStore\n",
    "from langchain_nomic.embeddings import NomicEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load documents\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "doc_list = [item for sublist in docs for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split documents\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Makara\\.conda\\envs\\ai-agents-env\\Lib\\site-packages\\sklearn\\base.py:493: FutureWarning: `BaseEstimator._check_feature_names` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_feature_names` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Makara\\.conda\\envs\\ai-agents-env\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Makara\\.conda\\envs\\ai-agents-env\\Lib\\site-packages\\sklearn\\base.py:411: FutureWarning: The `_get_tags` method is deprecated in 1.6 and will be removed in 1.7. Please implement the `__sklearn_tags__` method.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Makara\\.conda\\envs\\ai-agents-env\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# add to vectorDB\n",
    "vectorstore = SKLearnVectorStore.from_documents(\n",
    "    documents=doc_splits,\n",
    "    embedding=NomicEmbeddings(\n",
    "        model=\"nomic-embed-text-v1\",\n",
    "        inference_mode=\"local\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create retriever\n",
    "retriever = vectorstore.as_retriever(k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Components\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Router\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_core.messages import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt\n",
    "router_instructions = \"\"\"\n",
    "    You are an expert at routing a user question to a vectorstore or web search.\n",
    "    \n",
    "    The vectorstore contains documents related to agents, prompt engineering, and adversarial attacks.\n",
    "    \n",
    "    Use the vectorstore for questions on these topics. For all else, and especially for current events, use web-search.\n",
    "    \n",
    "    Return JSON with single key, datasource, that is 'websearch' or 'vectorstore' depending on the question.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test router\n",
    "test_web_search = llm_json_mode.invoke(\n",
    "    [SystemMessage(content=router_instructions)]\n",
    "    + [\n",
    "        HumanMessage(\n",
    "            content=\"Who is favored to win the NFC Championship game in the 2024 season?\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_web_search_2 = llm_json_mode.invoke(\n",
    "    [SystemMessage(content=router_instructions)]\n",
    "    + [HumanMessage(content=\"What are the models released today for llama 3.2?\")]\n",
    ")\n",
    "\n",
    "test_vector_store = llm_json_mode.invoke(\n",
    "    [SystemMessage(content=router_instructions)]\n",
    "    + [HumanMessage(content=\"What are the types of agent memory?\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.loads(test_web_search.content))\n",
    "print(json.loads(test_web_search_2.content))\n",
    "print(json.loads(test_vector_store.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieval Grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc grader instructions\n",
    "doc_grader_instructions = \"\"\"\n",
    "    You are a grader assessing relevence of a retrieved document to a user question.\n",
    "    If the document contains keyword(s) or sementic meaning related to the question, grade it as relevant.\n",
    "\"\"\"\n",
    "\n",
    "# grader prompt\n",
    "doc_grader_prompt = \"\"\"**Here is the retrieved document:** \\n\\n {document} \\n\\n **Here is the user question:** \\n\\n {question}.\\n\\n\n",
    "This carefully and objectively assess the document contains at least some information that is relevant to the question.\n",
    "Return JSON with single key, `binary_score`, that is 'yes' or 'no' score to indicate whether the document is contains at least some information that is relevant to the question.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Makara\\.conda\\envs\\ai-agents-env\\Lib\\site-packages\\sklearn\\base.py:493: FutureWarning: `BaseEstimator._check_feature_names` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_feature_names` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Makara\\.conda\\envs\\ai-agents-env\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "question = \"What is Chain of thought prompting?\"\n",
    "docs = retriever.invoke(question)\n",
    "doc_text = docs[1].page_content\n",
    "doc_grader_prompt_formatted = doc_grader_prompt.format(\n",
    "    document=doc_text, question=question\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Here is the retrieved document:** \n",
       "\n",
       " Use an iterative Monte Carlo search method to improve the best candidates by proposing semantically similar variants via prompts like Generate a variation of the following instruction while keeping the semantic meaning.\\n\\nInput: ...\\n\\nOutput:...\n",
       "\n",
       "\n",
       "To construct chain-of-thought prompts automatically, Shum et al. (2023) suggested augment-prune-select, a three-step process:\n",
       "\n",
       "Augment: Generate multiple pseudo-chains of thought given question using few-shot or zero-shot CoT prompts;\n",
       "Prune: Prune pseudo chains based on whether generated answers match ground truths.\n",
       "Select: Apply a variance-reduced policy gradient strategy to learn the probability distribution over selected examples, while considering the probability distribution over examples as policy and the validation set accuracy as reward.\n",
       "\n",
       "Zhang et al. (2023) instead adopted clustering techniques to sample questions and then generates chains. They observed that LLMs tend to make certain types of mistakes. One type of errors can be similar in the emebedding space and thus get grouped together. By only sampling one or a few from frequent-error clusters, we can prevent too many wrong demonstrations of one error type and collect a diverse set of examples.\n",
       "\n",
       "Question clustering: Embed questions and run $k$-means for clustering.\n",
       "Demonstration selection: Select a set of representative questions from each cluster; i.e. one demonstration from one cluster. Samples in each cluster are sorted by distance to the cluster centroid and those closer to the centroid are selected first.\n",
       "Rationale generation: Use zero-shot CoT to generate reasoning chains for selected questions and construct few-shot prompt to run inference.\n",
       "\n",
       "Augmented Language Models#\n",
       "A survey on augmented language models by Mialon et al. (2023) has great coverage over multiple categories of language models augmented with reasoning skills and the ability of using external tools. Recommend it.\n",
       "Retrieval#\n",
       "Often we need to complete tasks that require latest knowledge after the model pretraining time cutoff or internal/private knowledge base. In that case, the model would not know the context if we donâ€™t explicitly provide it in the prompt. Many methods for Open Domain Question Answering depend on first doing retrieval over a knowledge base and then incorporating the retrieved content as part of the prompt. The accuracy of such a process depends on the quality of both retrieval and generation steps.\n",
       "Lazaridou et al. (2022) studied how to use Google Search for document retrieval to augment LLMs. Given a question $q$, clean text is extracted out of 20 URLs returned by Google, resulting in a set of documents. Because these documents are long, each document is split into paragraphs of 6 sentences, $\\{p\\}$. Paragraphs are ranked by TF-IDF based cosine similarity between evidence paragraphs and the query. Only the most relevant paragraph is used in the prompt to produce an answer $a$.\n",
       "For closed-book QA, each demonstration is formatted as follows to construct few-shot prompts. Swapping the question with the evidence (longer distance between questions and answers) is found to consistently yield lower results across all datasets.\n",
       "Evidence: ...\n",
       "Question: ...\n",
       "Answer: ...\n",
       "The answer probability is computed in three ways:\n",
       "\n",
       "RAG style, $p(a_i \\mid q) = \\sum_{i=1}^n p_\\text{tf-idf} (p_i \\mid q) \\cdot p_\\text{LM}(a_i \\mid q, p_i)$, where $p_\\text{tf-idf} (p_i \\mid q)$ is the normalized cosine similarities between the TF-IDF passage and question representations.\n",
       "Noisy channel inference, $p(a_i\\mid q) = \\frac{p_\\text{LM}(q \\mid a_i, p_i) \\cdot p_\\text{LM}(a_i \\mid p_i)}{p_\\text{LM}(q \\mid p_i)}$\n",
       "Product-of-Experts (PoE), combines all probabilities used above in addition to $p_\\text{LM}(p_i \\mid q)$. \n",
       "\n",
       " **Here is the user question:** \n",
       "\n",
       " What is Chain of thought prompting?.\n",
       "\n",
       "\n",
       "This carefully and objectively assess the document contains at least some information that is relevant to the question.\n",
       "Return JSON with single key, `binary_score`, that is 'yes' or 'no' score to indicate whether the document is contains at least some information that is relevant to the question."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_md(doc_grader_prompt_formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'binary_score': 'yes'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# result\n",
    "result = llm_json_mode.invoke(\n",
    "    [SystemMessage(content=doc_grader_instructions)]\n",
    "    + [HumanMessage(content=doc_grader_prompt_formatted)]\n",
    ")\n",
    "\n",
    "json.loads(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt\n",
    "\n",
    "rag_prompt = \"\"\"You are an assistant for question-answering tasks.\n",
    "\n",
    "Here is the context to useto answer the question:\n",
    "{context}\n",
    "Think carefully about the context.\n",
    "\n",
    "Now, review the user question:\n",
    "{question}\n",
    "\n",
    "Provide an answer to this questions using only the above context.\n",
    "\n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "Answer:\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Makara\\.conda\\envs\\ai-agents-env\\Lib\\site-packages\\sklearn\\base.py:493: FutureWarning: `BaseEstimator._check_feature_names` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_feature_names` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Makara\\.conda\\envs\\ai-agents-env\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "docs = retriever.invoke(question)\n",
    "docs_txt = format_docs(docs)\n",
    "rag_prompt_formatted = rag_prompt.format(\n",
    "    context=docs_txt,\n",
    "    question=question\n",
    ")\n",
    "generation = llm.invoke([HumanMessage(content=rag_prompt_formatted)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Chain of thought prompting is a technique that elicits reasoning in large language models by generating multiple pseudo-chains of thought given a question. This process involves augmenting, pruning, and selecting examples to learn the probability distribution over selected examples, as suggested by Shum et al. (2023). The goal of chain of thought prompting is to improve the accuracy of complex questions by providing a step-by-step thought process, as seen in methods like Tree of Thoughts and Self-Ask."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_md(generation.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hallucination Grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "You are a teacher grading a quiz.\n",
       "You will be given `FACTS` and `STUDENT ANSWER`.\n",
       "\n",
       "\n",
       "\n",
       "Here is the grade criteria:\n",
       "\n",
       "1. Ensure the `STUDENT ANSWER` is grounded in the `FACTS`\n",
       "2. Ensure the `STUDENT ANSWER` does not contain \"hallucinated\" information outside the scope of the `FACTS`.\n",
       "\n",
       "\n",
       "\n",
       "Score:\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hallucination grader instructions\n",
    "hallucination_grader_instructions = \"\"\"You are a teacher grading a quiz.\n",
    "You will be given `FACTS` and `STUDENT ANSWER`.\\n\\n\n",
    "\n",
    "Here is the grade criteria:\\n\n",
    "1. Ensure the `STUDENT ANSWER` is grounded in the `FACTS`\n",
    "2. Ensure the `STUDENT ANSWER` does not contain \"hallucinated\" information outside the scope of the `FACTS`.\\n\\n\n",
    "\n",
    "Score:\n",
    "\"\"\"\n",
    "\n",
    "display_md(hallucination_grader_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "FACTS: \n",
       "\n",
       " {documents} \n",
       "\n",
       " STUDENT ANSWER: \n",
       "\n",
       " {generation}. \n",
       "\n",
       "\n",
       "Return JSON with two two keys, `binary_score` is 'yes' or 'no' score to indicate whether the `STUDENT ANSWER` is grounded in the `FACTS`. And a key, explanation, that contains a explanation of the score."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# grader prompt\n",
    "hallucination_grader_prompt = \"\"\"FACTS: \\n\\n {documents} \\n\\n STUDENT ANSWER: \\n\\n {generation}. \\n\\n\n",
    "Return JSON with two two keys, `binary_score` is 'yes' or 'no' score to indicate whether the `STUDENT ANSWER` is grounded in the `FACTS`. And a key, explanation, that contains a explanation of the score.\"\"\"\n",
    "\n",
    "display_md(hallucination_grader_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```json\n",
       "{\n",
       "  \"binary_score\": \"yes\",\n",
       "  \"explanation\": \"The student answer is grounded in the facts as it accurately describes chain of thought prompting and its goal, referencing relevant research by Shum et al. (2023) and mentioning specific methods like Tree of Thoughts and Self-Ask. The answer does not contain hallucinated information outside the scope of the provided facts.\"\n",
       "}\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test using documents and generation from above\n",
    "hallucination_grader_prompt_formatted = hallucination_grader_prompt.format(\n",
    "    documents=docs_txt,\n",
    "    generation=generation.content\n",
    ")\n",
    "\n",
    "result = llm_json_mode.invoke(\n",
    "    [SystemMessage(content=hallucination_grader_instructions)]\n",
    "    + [HumanMessage(content=hallucination_grader_prompt_formatted)]\n",
    ")\n",
    "display_json(json.loads(result.content))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-agents-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
